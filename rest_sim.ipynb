{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4ca285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/26 14:42:56 WARN Utils: Your hostname, ecs-syudosaev-big resolves to a loopback address: 127.0.1.1; using 10.11.12.124 instead (on interface eth0)\n",
      "24/05/26 14:42:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/26 14:42:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/26 14:42:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.types as st\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, FMClassifier\n",
    "from pyspark.ml import PipelineModel\n",
    "from sim4rec.modules import Simulator\n",
    "\n",
    "from replay.metrics import NDCG, Precision, RocAuc, Metric\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sim4rec.utils import VectorElementExtractor\n",
    "from sim4rec.modules import RealDataGenerator, SDVDataGenerator\n",
    "from sim4rec.modules import EvaluateMetrics\n",
    "from sim4rec.response import ParametricResponseFunction, BernoulliResponse\n",
    "\n",
    "from replay.models import UCB, ThompsonSampling\n",
    "from replay.models import RandomRec\n",
    "from replay.splitters import RandomSplitter\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName('simulator_movielens')\\\n",
    "    .master('local[*]')\\\n",
    "    .config('spark.sql.shuffle.partitions', '192')\\\n",
    "    .config('spark.default.parallelism', '192')\\\n",
    "    .config('spark.driver.extraJavaOptions', '-XX:+UseG1GC')\\\n",
    "    .config('spark.executor.extraJavaOptions', '-XX:+UseG1GC')\\\n",
    "    .config('spark.sql.autoBroadcastJoinThreshold', '-1')\\\n",
    "    .config('spark.driver.memory', '256g')\\\n",
    "    .config('spark.driver.maxResultSize', '256g')\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "def calc_metric(response_df):\n",
    "    return response_df.groupBy(\"user_idx\").agg(sf.sum(\"response\").alias(\"num_positive\")).select(sf.mean(\"num_positive\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cafd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.read.parquet('preprocessed/users.parquet')\n",
    "items_df = spark.read.parquet('preprocessed/items.parquet')\n",
    "log_df   = spark.read.parquet('preprocessed/rating.parquet')\n",
    "\n",
    "items_df = items_df.dropDuplicates(subset=['item_idx'])\n",
    "users_df = users_df.dropDuplicates(subset=['user_idx'])\n",
    "log_df = log_df.dropDuplicates(subset=['user_idx', 'item_idx'])\n",
    "\n",
    "items_df = items_df.withColumn('item_idx', sf.col('item_idx').cast('int'))\n",
    "users_df = users_df.withColumn('user_idx', sf.col('user_idx').cast('int'))\n",
    "log_df = log_df.withColumn('item_idx', sf.col('item_idx').cast('int'))\n",
    "log_df = log_df.withColumn('user_idx', sf.col('user_idx').cast('int'))\n",
    "\n",
    "log_df = log_df.join(users_df, log_df['user_idx'] == users_df['user_idx'], 'leftsemi')\n",
    "log_df = log_df.join(items_df, log_df['item_idx'] == items_df['item_idx'], 'leftsemi')\n",
    "\n",
    "for c in users_df.columns[1:]:\n",
    "    users_df = users_df.withColumnRenamed(c, 'user_' + c)\n",
    "\n",
    "for c in items_df.columns[1:]:\n",
    "    items_df = items_df.withColumnRenamed(c, 'item_' + c)\n",
    "\n",
    "users_df = users_df.cache()\n",
    "items_df = items_df.cache()\n",
    "log_df = log_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38019362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_dataframe = log_df.join(items_df, on='item_idx', how='inner').join(users_df, on='user_idx', how='inner').drop('timestamp')\n",
    "avg_item_ratings = full_dataframe.select('item_idx', 'relevance').groupBy('item_idx').agg(sf.mean(\"relevance\").alias('item_rating_avg'))\n",
    "full_dataframe = full_dataframe.join(avg_item_ratings, on='item_idx', how='inner')\n",
    "train_df, test_df = RandomSplitter(test_size=0.2, seed=9, drop_cold_items=True, drop_cold_users=True).split(full_dataframe)\n",
    "\n",
    "train_df = train_df.withColumn('relevance', sf.when(sf.col('relevance') >= 1, 1).otherwise(0))\n",
    "test_df = test_df.withColumn('relevance', sf.when(sf.col('relevance') >= 1, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117f73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_svd = spark.read.csv('item_svd.csv', header=True, inferSchema=True)\n",
    "# user_svd = spark.read.csv('user_svd.csv', header=True, inferSchema=True)\n",
    "\n",
    "# train_df = train_df.join(item_svd, on='item_idx', how='inner')\n",
    "# train_df = train_df.join(user_svd, on='user_idx', how='inner')\n",
    "\n",
    "# test_df = test_df.join(item_svd, on='item_idx', how='inner')\n",
    "# test_df = test_df.join(user_svd, on='user_idx', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56cd2385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train statistics\n",
      "639\n",
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "0.05873161764705882\n",
      "\n",
      "test statistics\n",
      "116\n",
      "71\n",
      "64\n",
      "0.025528169014084508\n"
     ]
    }
   ],
   "source": [
    "print('train statistics')\n",
    "print(train_df.count())\n",
    "print(train_df.select('user_idx').distinct().count())\n",
    "print(train_df.select('item_idx').distinct().count())\n",
    "print(train_df.count() / (train_df.select('user_idx').distinct().count() * train_df.select('item_idx').distinct().count()))\n",
    "print()\n",
    "\n",
    "print('test statistics')\n",
    "print(test_df.count())\n",
    "print(test_df.select('user_idx').distinct().count())\n",
    "print(test_df.select('item_idx').distinct().count())\n",
    "print(test_df.count() / (test_df.select('user_idx').distinct().count() * test_df.select('item_idx').distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f548d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(\n",
    "    inputCols=items_df.columns[1:] + users_df.columns[1:],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='relevance',\n",
    "    probabilityCol='proba'\n",
    ")\n",
    "fm = FMClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='relevance',\n",
    "    probabilityCol='proba'\n",
    ")\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='relevance',\n",
    "    probabilityCol='proba'\n",
    ")\n",
    "lr_model = lr.fit(va.transform(train_df))\n",
    "rf_model = rf.fit(va.transform(train_df))\n",
    "fm_model = fm.fit(va.transform(train_df))\n",
    "\n",
    "vee = VectorElementExtractor(inputCol='proba', outputCol='scores', index=1)\n",
    "mc = ParametricResponseFunction(inputCols=['scores'], outputCol='__pr', weights=[0.25])\n",
    "br = BernoulliResponse(inputCol='__pr', outputCol='response', seed=1234)\n",
    "pipeline_lr = PipelineModel(stages=[va, lr_model, vee, mc, br])\n",
    "pipeline_rf = PipelineModel(stages=[va, rf_model, vee, mc, br])\n",
    "pipeline_fm = PipelineModel(stages=[va, fm_model, vee, mc, br])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461b258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (baseline): 0.7902494331065759\n",
      "Precision (baseline): 0.9433962264150944\n",
      "Recall (baseline): 0.5102040816326531\n",
      "Accuracy (baseline): 0.5603448275862069\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (classificator): 0.8106575963718821\n",
      "Precision (classificator): 0.9354838709677419\n",
      "Recall (classificator): 0.8877551020408163\n",
      "Accuracy (classificator): 0.853448275862069\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (classificator): 0.778344671201814\n",
      "Precision (classificator): 0.8448275862068966\n",
      "Recall (classificator): 1.0\n",
      "Accuracy (classificator): 0.8448275862068966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4788:===================================================>(190 + 2) / 192]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (classificator): 0.6043083900226757\n",
      "Precision (classificator): 0.8777777777777778\n",
      "Recall (classificator): 0.8061224489795918\n",
      "Accuracy (classificator): 0.7413793103448276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def get_baseline(pred_df):\n",
    "    pred_df = pred_df.select('item_rating_avg', 'relevance').toPandas()\n",
    "    pred_df['baseline'] = (pred_df['item_rating_avg']-pred_df['item_rating_avg'].min())/(pred_df['item_rating_avg'].max()-pred_df['item_rating_avg'].min())\n",
    "    pred_df['baseline_bin'] = np.where(pred_df['baseline'] > 0.5, 1, 0) \n",
    "    print(f\"ROC AUC (baseline): {roc_auc_score(pred_df.relevance, pred_df.baseline)}\")\n",
    "    print(f\"Precision (baseline): {precision_score(pred_df.relevance, pred_df.baseline_bin)}\")\n",
    "    print(f\"Recall (baseline): {recall_score(pred_df.relevance, pred_df.baseline_bin)}\")\n",
    "    print(f\"Accuracy (baseline): {accuracy_score(pred_df.relevance, pred_df.baseline_bin)}\")\n",
    "    print()\n",
    "\n",
    "def assess_models(model, test_df):\n",
    "    pred_df = model.transform(test_df).select(\"relevance\", \"scores\").toPandas()\n",
    "    pred_df['response_bin'] = np.where(pred_df['scores'] > 0.5, 1, 0)\n",
    "    print(f\"ROC AUC (classificator): {roc_auc_score(pred_df.relevance, pred_df.scores)}\")\n",
    "    print(f\"Precision (classificator): {precision_score(pred_df.relevance, pred_df.response_bin)}\")\n",
    "    print(f\"Recall (classificator): {recall_score(pred_df.relevance, pred_df.response_bin)}\")\n",
    "    print(f\"Accuracy (classificator): {accuracy_score(pred_df.relevance, pred_df.response_bin)}\")\n",
    "    print()\n",
    "\n",
    "get_baseline(test_df)\n",
    "assess_models(pipeline_lr, test_df)\n",
    "assess_models(pipeline_rf, test_df)\n",
    "assess_models(pipeline_fm, test_df)\n",
    "\n",
    "# 0.7355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffede23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_lr = ThompsonSampling(sample = True)\n",
    "ts_lr.fit(log=train_df.drop('response').limit(1))\n",
    "\n",
    "ts_rf = ThompsonSampling(sample = True)\n",
    "ts_rf.fit(log=train_df.drop('response').limit(1))\n",
    "\n",
    "ts_fm = ThompsonSampling(sample = True)\n",
    "ts_fm.fit(log=train_df.drop('response').limit(1))\n",
    "\n",
    "random_uni = RandomRec(distribution=\"uniform\")\n",
    "random_uni.fit(log=train_df.drop('response').limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7e0cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = EvaluateMetrics(\n",
    "    userKeyCol='user_idx',\n",
    "    itemKeyCol='item_idx',\n",
    "    predictionCol='relevance',\n",
    "    labelCol='response',\n",
    "    replay_label_filter=1.0,\n",
    "    replay_metrics={NDCG() : 1, Precision() : 1, RocAuc(): 1}\n",
    ")\n",
    "\n",
    "users_generator = SDVDataGenerator(\n",
    "    label='synth',\n",
    "    id_column_name='user_id',\n",
    "    model_name='gaussiancopula',\n",
    "    parallelization_level=4,\n",
    "    device_name='cpu',\n",
    "    seed=1234\n",
    ")\n",
    "\n",
    "items_generator = RealDataGenerator(label='items_real', seed=1234)\n",
    "users_generator.fit(users_df)\n",
    "items_generator.fit(items_df)\n",
    "real_users = users_df.sample(1.0)\n",
    "syn_users = users_generator.generate(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb00496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sim4rec.modules.evaluation import evaluate_synthetic\n",
    "\n",
    "gen_score = evaluate_synthetic(\n",
    "    syn_users.drop('user_idx').drop('user_id'),\n",
    "    real_users.drop('user_idx')\n",
    ")\n",
    "gen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aedd6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of 50 iteration: \n",
      "35.52098202705383\n"
     ]
    }
   ],
   "source": [
    "def do_a_cycle(simul, model, pipeline, iteration, metrics):\n",
    "    users = simul.sample_users(1.0).cache()\n",
    "    log = simul.get_log(user_df=users)\n",
    "    log = train_df.drop('response').limit(1) if log is None else log\n",
    "\n",
    "    item_ids = items_df.select(\"item_idx\").sample(0.2).cache()\n",
    "    recs = model.predict(\n",
    "        log,\n",
    "        k=1,\n",
    "        users=users.select(\"user_idx\"),\n",
    "        items=item_ids,\n",
    "        filter_seen_items = False\n",
    "    )\n",
    "    resp = simul.sample_responses(\n",
    "        recs_df=recs, \n",
    "        user_features=users,\n",
    "        item_features=items_df,\n",
    "        action_models=pipeline,\n",
    "    ).select('user_idx', 'item_idx', 'relevance', 'response').cache()\n",
    "    simul.update_log(resp, iteration=iteration)\n",
    "    met = calc_metric(resp)\n",
    "    ev = evaluator(resp)\n",
    "    ev['CR'] = met\n",
    "    metrics.append(ev)\n",
    "\n",
    "    model._clear_cache()\n",
    "    train_log = simul.log.cache()\n",
    "    model.fit(train_log.select('user_idx', 'item_idx', 'response').withColumnRenamed('response', 'relevance'))\n",
    "\n",
    "    log.unpersist()\n",
    "    users.unpersist()\n",
    "    recs.unpersist()\n",
    "    resp.unpersist()\n",
    "    train_log.unpersist()\n",
    "\n",
    "sim_lr = Simulator(users_generator, items_generator, f'checkpoints/lr', None, 'user_idx', 'item_idx', spark)\n",
    "sim_rf = Simulator(users_generator, items_generator, f'checkpoints/rf', None, 'user_idx', 'item_idx', spark)\n",
    "sim_fm = Simulator(users_generator, items_generator, f'checkpoints/fm', None, 'user_idx', 'item_idx', spark)\n",
    "sim_rand = Simulator(users_generator, items_generator, f'checkpoints/rand', None, 'user_idx', 'item_idx', spark)\n",
    "\n",
    "lr_metrics = []\n",
    "rf_metrics = []\n",
    "fm_metrics = []\n",
    "rnd_metrics = []\n",
    "\n",
    "for i in range(50):\n",
    "    print(f'------------------------Stage {i}------------------------')\n",
    "    start_iter = time.time()\n",
    "\n",
    "    do_a_cycle(sim_lr, ts_lr, pipeline_lr, i, lr_metrics)\n",
    "    do_a_cycle(sim_rf, ts_rf, pipeline_rf, i, rf_metrics)\n",
    "    do_a_cycle(sim_fm, ts_fm, pipeline_fm, i, fm_metrics)\n",
    "    do_a_cycle(sim_rand, random_uni, pipeline_lr, i, rnd_metrics)\n",
    "    # plot_metric(lr_metrics)\n",
    "    clear_output(wait=True)\n",
    "    end_iter = time.time()\n",
    "    print(f\"Time of {i+1} iteration: \")\n",
    "    print(end_iter - start_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d456994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lr_metrics.pickle', 'wb') as f:\n",
    "    pickle.dump(lr_metrics, f)\n",
    "with open('rf_metrics.pickle', 'wb') as f:\n",
    "    pickle.dump(rf_metrics, f)\n",
    "with open('fm_metrics.pickle', 'wb') as f:\n",
    "    pickle.dump(fm_metrics, f)\n",
    "with open('rand_metrics.pickle', 'wb') as f:\n",
    "    pickle.dump(rnd_metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim4rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
